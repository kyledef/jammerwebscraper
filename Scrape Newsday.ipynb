{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The system will provide a simple example of using python to extract information from a website. Note this is part 1 of 4. Participants are not expected to have any experience in python or background in web scraping. However, some understanding of HTML will be useful.\n",
    "\n",
    "This draws inspiration from http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import dependencies (i.e. packages that extend the standard language to perform specific [advance] functionality)\n",
    "import urllib\n",
    "from datetime import datetime, date, timedelta\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From analysing the newsday archieve website we see that the URL follows a parsable convention\n",
    "http://www.newsday.co.tt/archives/YYYY-M-DD.html \n",
    "So our general approach will be as follows:\n",
    "    1. Generate date in the expected form between an ending and starting date\n",
    "    2. Test to ensure the dates generated are valid. (refine step1 based on results)\n",
    "    3. Read the content and process based on our goal for scaping the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1 - create a function to generates a list(array) of dates\n",
    "def genDates(start_date = date.today(), num_days = 3):\n",
    "    # date_list = [start - timedelta(days=x) for x in range(0, num_days)] # generate a list of dates\n",
    "    # While we expand the above line for beginners understanding \n",
    "    date_list = []\n",
    "    for d in range(0, num_days):\n",
    "        temp = start_date - timedelta(days=d)\n",
    "        date_list.append(temp.strftime('%Y-%-m-%d'))# http://strftime.org/ used a reference\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.newsday.co.tt/archives/2017-2-02\n",
      "http://www.newsday.co.tt/archives/2017-2-01\n",
      "http://www.newsday.co.tt/archives/2017-1-31\n"
     ]
    }
   ],
   "source": [
    "# Step 2 -Test the generated URL to ensure they point to \n",
    "def traverseDates(func, start_date = date.today(), num_days = 3):\n",
    "    base_url=\"http://www.newsday.co.tt/archives/\"\n",
    "    dates_str_list = genDates(start_date, num_days)\n",
    "    for date in dates_str_list:\n",
    "        url = base_url + date\n",
    "        func(url)\n",
    "        \n",
    "def printDate(date):\n",
    "    print(date)\n",
    "    \n",
    "traverseDates(printDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3 - Read content and process page\n",
    "def processPage(page_url):\n",
    "    print(\"Attempting to read content from {0}\".format(page_url))\n",
    "    page_content = urllib.urlopen(page_url).read()\n",
    "    beau = BeautifulSoup(page_content, \"html5lib\")\n",
    "    tables = beau.find_all(\"table\") #https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all\n",
    "    for i in range(0,13):\n",
    "        named_sec = tables[i].h3\n",
    "        if named_sec:\n",
    "            print(\"i {0} produced {1}\".format(i,named_sec))\n",
    "    article_links = beau.find_all(\"a\", 'title')\n",
    "    print(\"Found {0} tables and {1} articles\".format(len(tables), len(article_links)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# traverseDates(processPage,num_days = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Integrating IBM Watson\n",
    "import json\n",
    "from watson_developer_cloud import ToneAnalyzerV3\n",
    "from local_settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAnalyser():\n",
    "    tone_analyzer = ToneAnalyzerV3(\n",
    "       username= WATSON_CREDS['username'],\n",
    "       password= WATSON_CREDS['password'],\n",
    "       version='2016-05-19')\n",
    "    return tone_analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tone_analyzer.tone(text='A word is dead when it is said, some say. Emily Dickinson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analysePage(page_url):\n",
    "    page_content = urllib.urlopen(page_url).read()\n",
    "    beau = BeautifulSoup(page_content, \"html5lib\")\n",
    "    tables = beau.find_all(\"table\") #https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all\n",
    "    article_links = beau.find_all(\"a\", 'title')\n",
    "    print(\"Found {0} tables and {1} articles\".format(len(tables), len(article_links)))\n",
    "    for i in article_links:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# traverseDates(analysePage,num_days = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 tables and 39 articles\n"
     ]
    }
   ],
   "source": [
    "page_content = urllib.urlopen(\"http://www.newsday.co.tt/archives/2017-1-31\").read()\n",
    "beau = BeautifulSoup(page_content, \"html5lib\")\n",
    "tables = beau.find_all(\"table\") #https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all\n",
    "article_links = beau.find_all(\"a\", 'title')\n",
    "print(\"Found {0} tables and {1} articles\".format(len(tables), len(article_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processTone(tone):\n",
    "    large = tone[0]['score']\n",
    "    large_i = 0\n",
    "    for i in range(1, len(tone)):\n",
    "        if tone[i]['score'] > large:\n",
    "            large = tone[i]['score']\n",
    "            large_i = i\n",
    "    return tone[large_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ibm.com/watson/developercloud/tone-analyzer/api/v3/?python#post-tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first = True\n",
    "emo_count = {\n",
    "    \"anger\" : 0,\n",
    "    \"disgust\": 0,\n",
    "    \"fear\" : 0,\n",
    "    \"joy\" : 0,\n",
    "    \"sadness\": 0\n",
    "}\n",
    "socio_count = {\n",
    "    \"openness_big5\": 0,\n",
    "    \"conscientiousness_big5\": 0,\n",
    "    \"extraversion_big5\" : 0,\n",
    "    \"agreeableness_big5\" : 0,\n",
    "    \"emotional_range_big5\": 0\n",
    "}\n",
    "for i in article_links:\n",
    "    res = tone_analyzer.tone(i['title'])\n",
    "    tone = res['document_tone']['tone_categories']\n",
    "    emo = tone[0]['tones'] # we want the emotional tone\n",
    "    soci= tone[2]['tones'] # we also want the social tone\n",
    "    \n",
    "    e_res = processTone(emo)\n",
    "    emo_count[e_res['tone_id']] += 1\n",
    "    \n",
    "    s_res = processTone(soci)\n",
    "    socio_count[s_res['tone_id']] += 1\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 articles were classified with the emotion anger\n",
      "11 articles were classified with the emotion joy\n",
      "4 articles were classified with the emotion fear\n",
      "15 articles were classified with the emotion sadness\n",
      "6 articles were classified with the emotion disgust\n"
     ]
    }
   ],
   "source": [
    "for e in emo_count:\n",
    "    print(\"{0} articles were classified with the emotion {1}\".format(emo_count[e], e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 articles were classified as extraversion_big5\n",
      "5 articles were classified as openness_big5\n",
      "14 articles were classified as conscientiousness_big5\n",
      "5 articles were classified as agreeableness_big5\n",
      "4 articles were classified as emotional_range_big5\n"
     ]
    }
   ],
   "source": [
    "for s in socio_count:\n",
    "    print(\"{0} articles were classified as {1}\".format(socio_count[s], s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
