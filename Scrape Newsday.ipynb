{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping in Python Series\n",
    "\n",
    "## Introduction\n",
    "The system will provide a simple example of using python to extract information from a website. Note this is part 1 of 4. Participants are not expected to have any experience in python or background in web scraping. However, some understanding of HTML will be useful.\n",
    "\n",
    "This draws inspiration from http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import dependencies (i.e. packages that extend the standard language to perform specific [advance] functionality)\n",
    "import urllib\n",
    "from datetime import datetime, date, timedelta\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From analysing the newsday archieve website we see that the URL follows a parsable convention\n",
    "http://www.newsday.co.tt/archives/YYYY-M-DD.html \n",
    "So our general approach will be as follows:\n",
    "    1. Generate date in the expected form between an ending and starting date\n",
    "    2. Test to ensure the dates generated are valid. (refine step1 based on results)\n",
    "    3. Read the content and process based on our goal for scaping the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1 - create a function to generates a list(array) of dates\n",
    "def genDates(start_date = date.today(), num_days = 3):\n",
    "    # date_list = [start - timedelta(days=x) for x in range(0, num_days)] # generate a list of dates\n",
    "    # While we expand the above line for beginners understanding \n",
    "    date_list = []\n",
    "    for d in range(0, num_days):\n",
    "        temp = start_date - timedelta(days=d)\n",
    "        date_list.append(temp.strftime('%Y-%-m-%d'))# http://strftime.org/ used a reference\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.newsday.co.tt/archives/2017-2-02\n",
      "http://www.newsday.co.tt/archives/2017-2-01\n",
      "http://www.newsday.co.tt/archives/2017-1-31\n"
     ]
    }
   ],
   "source": [
    "# Step 2 -Test the generated URL to ensure they point to \n",
    "def traverseDates(func, start_date = date.today(), num_days = 3):\n",
    "    base_url=\"http://www.newsday.co.tt/archives/\"\n",
    "    dates_str_list = genDates(start_date, num_days)\n",
    "    for date in dates_str_list:\n",
    "        url = base_url + date\n",
    "        func(url)\n",
    "        \n",
    "def printDate(date):\n",
    "    print(date)\n",
    "    \n",
    "traverseDates(printDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3 - Read content and process page\n",
    "def processPage(page_url):\n",
    "    print(\"Attempting to read content from {0}\".format(page_url))\n",
    "    page_content = urllib.urlopen(page_url).read()\n",
    "    beau = BeautifulSoup(page_content, \"html5lib\")\n",
    "    tables = beau.find_all(\"table\") #https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all\n",
    "    for i in range(0,13):\n",
    "        named_sec = tables[i].h3\n",
    "        if named_sec:\n",
    "            print(\"i {0} produced {1}\".format(i,named_sec))\n",
    "    article_links = beau.find_all(\"a\", 'title')\n",
    "    print(\"Found {0} tables and {1} articles\".format(len(tables), len(article_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# traverseDates(processPage,num_days = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Purpose (Goal) of Scraping\n",
    "Our main purpose of developing this exercise was to determine if the statement that the majority of the news published was negative. To do this we need to capture the sentiment of the information extracted from the link. While we can develop sentiment analysis tools using python, the process of training and validating is too much work at this time. Therefore, we utilize the IBM Watson Tone Analyzer API. We selected this API because it provides a greater amount of detail rather than a binary positive or negative result.\n",
    "\n",
    "To use the watson api for python:\n",
    "\n",
    "1. We installed the pip pacakge\n",
    "    ```bash\n",
    "    pip install --upgrade watson-developer-cloud\n",
    "    ```\n",
    "2. We created an account (free for 30 days)\n",
    "    https://tone-analyzer-demo.mybluemix.net/\n",
    "    \n",
    "3. Use the API referene to build application\n",
    "    http://www.ibm.com/watson/developercloud/tone-analyzer/api/v3/?python#\n",
    "4. Created a local_settings.py file that contains the credentials retrieved from signing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Integrating IBM Watson\n",
    "import json\n",
    "from watson_developer_cloud import ToneAnalyzerV3\n",
    "from local_settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAnalyser():\n",
    "    tone_analyzer = ToneAnalyzerV3(\n",
    "       username= WATSON_CREDS['username'],\n",
    "       password= WATSON_CREDS['password'],\n",
    "       version='2016-05-19')\n",
    "    return tone_analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tone_analyzer = getAnalyser()\n",
    "# tone_analyzer.tone(text='A word is dead when it is said, some say. Emily Dickinson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analysePage(page_url):\n",
    "    page_content = urllib.urlopen(page_url).read()\n",
    "    beau = BeautifulSoup(page_content, \"html5lib\")\n",
    "    tables = beau.find_all(\"table\") #https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all\n",
    "    article_links = beau.find_all(\"a\", 'title')\n",
    "    print(\"Found {0} tables and {1} articles\".format(len(tables), len(article_links)))\n",
    "    for i in article_links:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# traverseDates(analysePage,num_days = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13 tables and 39 articles\n"
     ]
    }
   ],
   "source": [
    "page_content = urllib.urlopen(\"http://www.newsday.co.tt/archives/2017-2-2\").read()\n",
    "beau = BeautifulSoup(page_content, \"html5lib\")\n",
    "tables = beau.find_all(\"table\") #https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all\n",
    "article_links = beau.find_all(\"a\", 'title')\n",
    "print(\"Found {0} tables and {1} articles\".format(len(tables), len(article_links)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processTone(tone):\n",
    "    large = tone[0]['score']\n",
    "    large_i = 0\n",
    "    for i in range(1, len(tone)):\n",
    "        if tone[i]['score'] > large:\n",
    "            large = tone[i]['score']\n",
    "            large_i = i\n",
    "    return tone[large_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Understanding of the structure of the response is provided in the API reference\n",
    "\n",
    "https://www.ibm.com/watson/developercloud/tone-analyzer/api/v3/?python#post-tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first = True\n",
    "emo_count = {\n",
    "    \"anger\" : 0,\n",
    "    \"disgust\": 0,\n",
    "    \"fear\" : 0,\n",
    "    \"joy\" : 0,\n",
    "    \"sadness\": 0\n",
    "}\n",
    "socio_count = {\n",
    "    \"openness_big5\": 0,\n",
    "    \"conscientiousness_big5\": 0,\n",
    "    \"extraversion_big5\" : 0,\n",
    "    \"agreeableness_big5\" : 0,\n",
    "    \"emotional_range_big5\": 0\n",
    "}\n",
    "for i in article_links:\n",
    "    res = tone_analyzer.tone(i['title'])\n",
    "    tone = res['document_tone']['tone_categories']\n",
    "    emo = tone[0]['tones'] # we want the emotional tone\n",
    "    soci= tone[2]['tones'] # we also want the social tone\n",
    "    \n",
    "    e_res = processTone(emo)\n",
    "    emo_count[e_res['tone_id']] += 1\n",
    "    \n",
    "    s_res = processTone(soci)\n",
    "    socio_count[s_res['tone_id']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 articles were classified with the emotion anger\n",
      "11 articles were classified with the emotion joy\n",
      "4 articles were classified with the emotion fear\n",
      "15 articles were classified with the emotion sadness\n",
      "6 articles were classified with the emotion disgust\n"
     ]
    }
   ],
   "source": [
    "for e in emo_count:\n",
    "    print(\"{0} articles were classified with the emotion {1}\".format(emo_count[e], e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 articles were classified as extraversion_big5\n",
      "5 articles were classified as openness_big5\n",
      "14 articles were classified as conscientiousness_big5\n",
      "5 articles were classified as agreeableness_big5\n",
      "4 articles were classified as emotional_range_big5\n"
     ]
    }
   ],
   "source": [
    "for s in socio_count:\n",
    "    print(\"{0} articles were classified as {1}\".format(socio_count[s], s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
